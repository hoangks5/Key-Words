CHAPTER 2. LINEAR ALGEBRAFor example, if a matrix is not square, the eigendecomposition is not deﬁned, andwe must use a singular value decomposition instead.Recall that the eigendecomposition involves analyzing a matrixAto discovera matrixVof eigenvectors and a vector of eigenvaluesλsuch that we can rewriteA asA = V diag(λ)V−1. (2.42)The singular value decomposition is similar, except this time we will writeAas a product of three matrices:A = U DV. (2.43)Suppose thatAis anm ×nmatrix. ThenUis deﬁned to be anm ×mmatrix,D to be an m × n matrix, and V to be an n × n matrix.Each of these matrices is deﬁned to have a special structure. The matricesUandVare both deﬁned to be orthogonal matrices. The matrixDis deﬁned to bea diagonal matrix. Note that D is not necessarily square.The elements along the diagonal ofDare known as thesingular valuesofthe matrixA. The columns ofUare known as theleft-singular vectors. Thecolumns of V are known as as the right-singular vectors.We can actually interpret the singular value decomposition ofAin terms ofthe eigendecomposition of functions ofA. The left-singular vectors ofAare theeigenvectors ofAA. The right-singular vectors ofAare the eigenvectors ofAA.The nonzero singular values ofAare the square roots of the eigenvalues ofAA.The same is true for AA.Perhaps the most useful feature of the SVD is that we can use it to partiallygeneralize matrix inversion to nonsquare matrices, as we will see in the next section.2.9 The Moore-Penrose PseudoinverseMatrix inversion is not deﬁned for matrices that are not square. Suppose we wantto make a left-inverse B of a matrix A so that we can solve a linear equationAx = y (2.44)by left-multiplying each side to obtainx = By. (2.45)43
CHAPTER 2. LINEAR ALGEBRADepending on the structure of the problem, it may not be possible to design aunique mapping from A to B.IfAis taller than it is wide, then it is possible for this equation to haveno solution. IfAis wider than it is tall, then there could be multiple possiblesolutions.TheMoore-Penrose pseudoinverseenables us to make some headway inthese cases. The pseudoinverse of A is deﬁned as a matrixA+= limα0(AA + αI)−1A. (2.46)Practical algorithms for computing the pseudoinverse are based not on this deﬁni-tion, but rather on the formulaA+= V D+U, (2.47)whereU,DandVare the singular value decomposition ofA, and the pseudoinverseD+of a diagonal matrixDis obtained by taking the reciprocal of its nonzeroelements then taking the transpose of the resulting matrix.WhenAhas more columns than rows, then solving a linear equation using thepseudoinverse provides one of the many possible solutions. Speciﬁcally, it providesthe solutionx=A+ywith minimal Euclidean norm||x||2among all possiblesolutions.WhenAhas more rows than columns, it is possible for there to be no solution.In this case, using the pseudoinverse gives us thexfor whichAxis as close aspossible to y in terms of Euclidean norm ||Ax − y||2.2.10 The Trace OperatorThe trace operator gives the sum of all the diagonal entries of a matrix:Tr(A) =iAi,i. (2.48)The trace operator is useful for a variety of reasons. Some operations that arediﬃcult to specify without resorting to summation notation can be speciﬁed usingmatrix products and the trace operator. For example, the trace operator providesan alternative way of writing the Frobenius norm of a matrix:||A||F=Tr(AA). (2.49)44
CHAPTER 2. LINEAR ALGEBRAWriting an expression in terms of the trace operator opens up opportunities tomanipulate the expression using many useful identities. For example, the traceoperator is invariant to the transpose operator:Tr(A) = Tr(A). (2.50)The trace of a square matrix composed of many factors is also invariant tomoving the last factor into the ﬁrst position, if the shapes of the correspondingmatrices allow the resulting product to be deﬁned:Tr(ABC) = Tr(CAB) = Tr(BCA) (2.51)or more generally,Tr(ni=1F(i)) = Tr(F(n)n−1i=1F(i)). (2.52)This invariance to cyclic permutation holds even if the resulting product has adiﬀerent shape. For example, for A ∈ Rm×nand B ∈ Rn×m, we haveTr(AB) = Tr(BA) (2.53)even though AB ∈ Rm×mand BA ∈ Rn×n.Another useful fact to keep in mind is that a scalar is its own trace:a=Tr(a).2.11 The DeterminantThe determinant of a square matrix, denoteddet(A), is a function that mapsmatrices to real scalars. The determinant is equal to the product of all theeigenvalues of the matrix. The absolute value of the determinant can be thoughtof as a measure of how much multiplication by the matrix expands or contractsspace. If the determinant is 0, then space is contracted completely along at leastone dimension, causing it to lose all its volume. If the determinant is 1, then thetransformation preserves volume.2.12 Example: Principal Components AnalysisOne simple machine learning algorithm,principal components analysis(PCA),can be derived using only knowledge of basic linear algebra.45
CHAPTER 2. LINEAR ALGEBRASuppose we have a collection of m points {x(1), . . . , x(m)} in Rnand we wantto apply lossy compression to these points. Lossy compression means storing thepoints in a way that requires less memory but may lose some precision. We wantto lose as little precision as possible.One way we can encode these points is to represent a lower-dimensional versionof them. For each pointx(i)∈ Rnwe will ﬁnd a corresponding code vectorc(i)∈ Rl.Iflis smaller thann, storing the code points will take less memory than storing theoriginal data. We will want to ﬁnd some encoding function that produces the codefor an input,f(x) =c, and a decoding function that produces the reconstructedinput given its code, x ≈ g(f (x)).PCA is deﬁned by our choice of the decoding function. Speciﬁcally, to make thedecoder very simple, we choose to use matrix multiplication to map the code backinto Rn. Let g(c) = Dc, where D ∈ Rn×lis the matrix deﬁning the decoding.Computing the optimal code for this decoder could be a diﬃcult problem. Tokeep the encoding problem easy, PCA constrains the columns ofDto be orthogonalto each other. (Note thatDis still not technically “an orthogonal matrix” unlessl = n.)With the problem as described so far, many solutions are possible, because wecan increase the scale ofD:,iif we decreaseciproportionally for all points. To givethe problem a unique solution, we constrain all the columns ofDto have unitnorm.In order to turn this basic idea into an algorithm we can implement, the ﬁrstthing we need to do is ﬁgure out how to generate the optimal code pointc∗foreach input pointx. One way to do this is to minimize the distance between theinput pointxand its reconstruction,g(c∗). We can measure this distance using anorm. In the principal components algorithm, we use the L2norm:c∗= arg minc||x − g(c)||2. (2.54)We can switch to the squaredL2norm instead of using theL2norm itselfbecause both are minimized by the same value ofc. Both are minimized by thesame value ofcbecause theL2norm is non-negative and the squaring operation ismonotonically increasing for non-negative arguments.c∗= arg minc||x − g(c)||22. (2.55)The function being minimized simpliﬁes to(x − g(c))(x − g(c)) (2.56)46
CHAPTER 2. LINEAR ALGEBRA(by the deﬁnition of the L2norm, equation 2.30)= xx − xg(c) −g(c)x + g(c)g(c) (2.57)(by the distributive property)= xx − 2xg(c) + g(c)g(c) (2.58)(because the scalar g(c)x is equal to the transpose of itself).We can now change the function being minimized again, to omit the ﬁrst term,since this term does not depend on c:c∗= arg minc−2xg(c) + g(c)g(c). (2.59)To make further progress, we must substitute in the deﬁnition of g(c):c∗= arg minc−2xDc + cDDc (2.60)= arg minc−2xDc + cIlc (2.61)(by the orthogonality and unit norm constraints on D)= arg minc−2xDc + cc. (2.62)We can solve this optimization problem using vector calculus (see section 4.3 ifyou do not know how to do this):∇c(−2xDc + cc) = 0 (2.63)− 2Dx + 2c = 0 (2.64)c = Dx. (2.65)This makes the algorithm eﬃcient: we can optimally encodexusing just amatrix-vector operation. To encode a vector, we apply the encoder functionf(x) = Dx. (2.66)Using a further matrix multiplication, we can also deﬁne the PCA reconstructionoperation:r(x) = g (f (x)) = DDx. (2.67)47