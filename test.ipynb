{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt','r',encoding='utf-8') as file:\n",
    "    full_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyphrase:  Oraichain Academy : score 0.00509403510229188\n",
      "Keyphrase:  Oraichain : score 0.012661539516886469\n",
      "Keyphrase:  Oraichain Academy journal : score 0.0216785908371288\n",
      "Keyphrase:  Academy : score 0.024160619431243494\n",
      "Keyphrase:  managed Oraichain Team : score 0.02622825036167718\n",
      "Keyphrase:  managed Oraichain : score 0.049338869717212734\n",
      "Keyphrase:  distribute Oraichain : score 0.049338869717212734\n",
      "Keyphrase:  Oraichain Team : score 0.049750667762045064\n",
      "Keyphrase:  Organized and managed : score 0.05352915320746283\n",
      "Keyphrase:  technology : score 0.05430182605321137\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "keywords = kw_extractor.extract_keywords(full_text)\n",
    "for kw, v in keywords:\n",
    "  print(\"Keyphrase: \",kw, \": score\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('create good content', 8.5), ('gain valuable knowledge', 8.0), ('main topic categories', 8.0), ('system design decisions', 8.0), ('managed oraichain team', 7.642857142857142), ('distribute oraichain’s research', 7.5), ('communicate oraichain’s technology', 6.571428571428571), ('important crypto events', 6.333333333333334), ('oraichain academy journal', 6.267857142857142), ('oraichain team', 4.642857142857142)]\n"
     ]
    }
   ],
   "source": [
    "from multi_rake import Rake\n",
    "rake = Rake()\n",
    "keywords = rake.apply(full_text)\n",
    "print(keywords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('oraichain', 0.4148995819705745), ('technology', 0.20197149537045767), ('technologies', 0.20197149537045767), ('topics', 0.17754749033140257), ('topic', 0.17754749033140257), ('crypto', 0.1666449094774695), ('good', 0.1552571681022481), ('research', 0.1497464347703595), ('researchers', 0.1497464347703595), ('effects', 0.1461832029297981)]\n"
     ]
    }
   ],
   "source": [
    "from summa import keywords\n",
    "TR_keywords = keywords.keywords(full_text, scores=True)\n",
    "print(TR_keywords[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contribution oraichain ecosystem', 'oraichain research views', 'oraichain technology articles', 'oraichain technology', 'communicate oraichain technology', 'oracle blockchain ecosystem', 'oraichain ecosystem opinions', 'announcing purposes oraichain', 'valuable knowledge oraichain', 'distribute oraichain research']\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "keywords = kw_model.extract_keywords(full_text, \n",
    "\n",
    "                                     keyphrase_ngram_range=(1, 3), \n",
    "\n",
    "                                     stop_words='english',\n",
    "\n",
    "                                     highlight=False,\n",
    "\n",
    "                                     top_n=10)\n",
    "\n",
    "keywords_list= list(dict(keywords).keys())\n",
    "print(keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([full_text])\n",
    "candidates = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([full_text])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['innovation', 'researchers', 'scientists', 'blog', 'online']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
